uvicorn app.main:app --reload --host 127.0.0.1 --port 8000





# @app.post("/upload")
# async def upload_dataset(
#     file: UploadFile = File(...),
#     user_id: str = Query(..., description="User identifier"),
#     auto_commit: bool = Query(False, description="Auto-commit to Solana")
# ):
#     """
#     Upload and clean dataset
#     Returns: cleaned file + dataset hash
#     """
#     try:
#         # Validate file type
#         if not file.filename.endswith(('.csv', '.json', '.xlsx', '.xls')):
#             raise HTTPException(400, "Unsupported file format. Use CSV, JSON, or XLSX")
        
#         # Read file content
#         content = await file.read()
        
#         # Load dataset
#         if file.filename.endswith('.csv'):
#             df = pd.read_csv(io.BytesIO(content))
#         elif file.filename.endswith('.json'):
#             df = pd.read_json(io.BytesIO(content))
#         elif file.filename.endswith(('.xlsx', '.xls')):
#             df = pd.read_excel(io.BytesIO(content))
        
#         # Clean dataset
#         cleaned_df, cleaning_report = cleaner.clean(df)
        
#         # Generate unique ID and hash
#         dataset_id = str(uuid.uuid4())
#         cleaned_csv = cleaned_df.to_csv(index=False)
#         dataset_hash = hashlib.sha256(cleaned_csv.encode()).hexdigest()
        
#         # Save cleaned file
#         cleaned_filename = f"{dataset_id}_cleaned.csv"
#         cleaned_path = CLEANED_DIR / cleaned_filename
#         cleaned_df.to_csv(cleaned_path, index=False)
        
#         # Store metadata in database
#         metadata = DatasetMetadata(
#             id=dataset_id,
#             user_id=user_id,
#             original_filename=file.filename,
#             cleaned_filename=cleaned_filename,
#             dataset_hash=dataset_hash,
#             rows_original=len(df),
#             rows_cleaned=len(cleaned_df),
#             columns=len(cleaned_df.columns),
#             cleaning_report=cleaning_report,
#             created_at=datetime.utcnow(),
#             committed_to_solana=False
#         )
        
#         await db.insert_metadata(metadata)
        
#         # Update user stats
#         dataset_size = len(content)
#         update_user_stats(user_id, dataset_size)
        
#         # Auto-commit to Solana if requested
#         solana_signature = None
#         if auto_commit:
#             try:
#                 solana_signature = await solana_client.commit_hash(
#                     dataset_hash, user_id, int(datetime.utcnow().timestamp())
#                 )
#                 await db.update_solana_status(dataset_id, True, solana_signature)
#             except Exception as e:
#                 print(f"⚠️ Solana commit failed: {e}")
        
#         return JSONResponse({
#             "success": True,
#             "dataset_id": dataset_id,
#             "dataset_hash": dataset_hash,
#             "original_rows": len(df),
#             "cleaned_rows": len(cleaned_df),
#             "columns": list(cleaned_df.columns),
#             "cleaning_report": cleaning_report,
#             "download_url": f"/download/{dataset_id}",
#             "solana_signature": solana_signature,
#             "committed_to_solana": auto_commit and solana_signature is not None
#         })
        
#     except pd.errors.EmptyDataError:
#         raise HTTPException(400, "Empty dataset")
#     except Exception as e:
#         raise HTTPException(500, f"Processing error: {str(e)}")

# @app.post("/upload")
# async def upload_dataset(
#     file: UploadFile = File(...),
#     user_id: str = Query(..., description="User identifier"),
#     auto_commit: bool = Query(False, description="Auto-commit to Solana")
# ):
#     try:
#         # Validate file type
#         if not file.filename.endswith(('.csv', '.json', '.xlsx', '.xls')):
#             raise HTTPException(400, "Unsupported file format. Use CSV, JSON, or XLSX")
        
#         # Read file content
#         content = await file.read()
#         file_size = len(content)  # ACTUAL FILE SIZE IN BYTES
        
#         # Load dataset
#         if file.filename.endswith('.csv'):
#             df = pd.read_csv(io.BytesIO(content))
#         elif file.filename.endswith('.json'):
#             df = pd.read_json(io.BytesIO(content))
#         elif file.filename.endswith(('.xlsx', '.xls')):
#             df = pd.read_excel(io.BytesIO(content))
        
#         # Clean dataset
#         cleaned_df, cleaning_report = cleaner.clean(df)
        
#         # Generate unique ID and hash
#         dataset_id = str(uuid.uuid4())
#         cleaned_csv = cleaned_df.to_csv(index=False)
#         cleaned_size = len(cleaned_csv.encode())  # CLEANED SIZE IN BYTES
#         dataset_hash = hashlib.sha256(cleaned_csv.encode()).hexdigest()
        
#         # Save cleaned file
#         cleaned_filename = f"{dataset_id}_cleaned.csv"
#         cleaned_path = CLEANED_DIR / cleaned_filename
#         cleaned_df.to_csv(cleaned_path, index=False)
        
#         # Store metadata in database
#         metadata = DatasetMetadata(
#             id=dataset_id,
#             user_id=user_id,
#             original_filename=file.filename,
#             cleaned_filename=cleaned_filename,
#             dataset_hash=dataset_hash,
#             rows_original=len(df),
#             rows_cleaned=len(cleaned_df),
#             columns=len(cleaned_df.columns),
#             cleaning_report=cleaning_report,
#             created_at=datetime.utcnow(),
#             committed_to_solana=False
#         )
        
#         await db.insert_metadata(metadata)
        
#         # Update user stats with ACTUAL file size
#         update_user_stats(user_id, file_size)
        
#         # Auto-commit to Solana if requested
#         solana_signature = None
#         if auto_commit:
#             try:
#                 solana_signature = await solana_client.commit_hash(
#                     dataset_hash, user_id, int(datetime.utcnow().timestamp())
#                 )
#                 await db.update_solana_status(dataset_id, True, solana_signature)
#             except Exception as e:
#                 print(f"⚠️ Solana commit failed: {e}")
        
#         # Format file sizes properly
#         def format_bytes(bytes_val):
#             if bytes_val < 1024:
#                 return f"{bytes_val} B"
#             elif bytes_val < 1024 * 1024:
#                 return f"{bytes_val / 1024:.2f} KB"
#             else:
#                 return f"{bytes_val / (1024 * 1024):.2f} MB"
        
#         return JSONResponse({
#             "success": True,
#             "dataset_id": dataset_id,
#             "dataset_hash": dataset_hash,
#             "original_rows": len(df),
#             "cleaned_rows": len(cleaned_df),
#             "columns": list(cleaned_df.columns),
#             "file_size": format_bytes(file_size),  # NEW: Formatted size
#             "cleaned_size": format_bytes(cleaned_size),  # NEW: Cleaned size
#             "file_size_bytes": file_size,  # NEW: Raw bytes for frontend
#             "cleaned_size_bytes": cleaned_size,  # NEW: Raw bytes for frontend
#             "cleaning_report": cleaning_report,
#             "download_url": f"/download/{dataset_id}",
#             "solana_signature": solana_signature,
#             "committed_to_solana": auto_commit and solana_signature is not None
#         })
        
#     except pd.errors.EmptyDataError:
#         raise HTTPException(400, "Empty dataset")
#     except Exception as e:
#         raise HTTPException(500, f"Processing error: {str(e)}")
    
# @app.post("/upload")
# async def upload_dataset(
#     file: UploadFile = File(...),
#     user_id: str = Query(..., description="User identifier"),
#     auto_commit: bool = Query(False, description="Auto-commit to Solana")
# ):
#     try:
#         # Validate file type
#         if not file.filename.endswith(('.csv', '.json', '.xlsx', '.xls')):
#             raise HTTPException(400, "Unsupported file format. Use CSV, JSON, or XLSX")
        
#         # Read file content
#         content = await file.read()
#         file_size = len(content)
        
#         # Load dataset
#         if file.filename.endswith('.csv'):
#             df = pd.read_csv(io.BytesIO(content))
#         elif file.filename.endswith('.json'):
#             df = pd.read_json(io.BytesIO(content))
#         elif file.filename.endswith(('.xlsx', '.xls')):
#             df = pd.read_excel(io.BytesIO(content))
        
#         # Clean dataset
#         cleaned_df, cleaning_report = cleaner.clean(df)
        
#         # Generate hash FIRST (before ID)
#         cleaned_csv = cleaned_df.to_csv(index=False)
#         cleaned_size = len(cleaned_csv.encode())
#         dataset_hash = hashlib.sha256(cleaned_csv.encode()).hexdigest()
        
#         # CHECK IF HASH ALREADY EXISTS
#         existing_metadata = None
#         if db.use_sqlite:
#             cursor = db.sqlite_conn.cursor()
#             cursor.execute('SELECT * FROM datasets WHERE dataset_hash = ?', (dataset_hash,))
#             existing_row = cursor.fetchone()
#             if existing_row:
#                 # Hash exists, return existing data
#                 existing_metadata = {
#                     "id": existing_row['id'],
#                     "dataset_hash": existing_row['dataset_hash'],
#                     "original_rows": existing_row['rows_original'],
#                     "cleaned_rows": existing_row['rows_cleaned'],
#                     "columns": existing_row['columns'],
#                     "cleaning_report": json.loads(existing_row['cleaning_report']),
#                     "download_url": f"/download/{existing_row['id']}",
#                     "solana_signature": existing_row['solana_signature'],
#                     "committed_to_solana": bool(existing_row['committed_to_solana'])
#                 }
#         else:
#             # PostgreSQL
#             if db.pool:
#                 async with db.pool.acquire() as conn:
#                     existing_row = await conn.fetchrow(
#                         'SELECT * FROM datasets WHERE dataset_hash = $1', dataset_hash
#                     )
#                     if existing_row:
#                         existing_metadata = {
#                             "id": existing_row['id'],
#                             "dataset_hash": existing_row['dataset_hash'],
#                             "original_rows": existing_row['rows_original'],
#                             "cleaned_rows": existing_row['rows_cleaned'],
#                             "columns": existing_row['columns'],
#                             "cleaning_report": json.loads(existing_row['cleaning_report']),
#                             "download_url": f"/download/{existing_row['id']}",
#                             "solana_signature": existing_row['solana_signature'],
#                             "committed_to_solana": existing_row['committed_to_solana']
#                         }
        
#         if existing_metadata:
#             # Return existing dataset info
#             def format_bytes(bytes_val):
#                 if bytes_val < 1024:
#                     return f"{bytes_val} B"
#                 elif bytes_val < 1024 * 1024:
#                     return f"{bytes_val / 1024:.2f} KB"
#                 else:
#                     return f"{bytes_val / (1024 * 1024):.2f} MB"
            
#             return JSONResponse({
#                 "success": True,
#                 "message": "Dataset already exists (duplicate)",
#                 "dataset_id": existing_metadata['id'],
#                 "dataset_hash": existing_metadata['dataset_hash'],
#                 "original_rows": existing_metadata['original_rows'],
#                 "cleaned_rows": existing_metadata['cleaned_rows'],
#                 "columns": existing_metadata['columns'],
#                 "file_size": format_bytes(file_size),
#                 "cleaned_size": format_bytes(cleaned_size),
#                 "file_size_bytes": file_size,
#                 "cleaned_size_bytes": cleaned_size,
#                 "cleaning_report": existing_metadata['cleaning_report'],
#                 "download_url": existing_metadata['download_url'],
#                 "solana_signature": existing_metadata['solana_signature'],
#                 "committed_to_solana": existing_metadata['committed_to_solana']
#             })
        
#         # Generate unique ID (only if hash doesn't exist)
#         dataset_id = str(uuid.uuid4())
        
#         # Save cleaned file
#         cleaned_filename = f"{dataset_id}_cleaned.csv"
#         cleaned_path = CLEANED_DIR / cleaned_filename
#         cleaned_df.to_csv(cleaned_path, index=False)
        
#         # Store metadata in database
#         metadata = DatasetMetadata(
#             id=dataset_id,
#             user_id=user_id,
#             original_filename=file.filename,
#             cleaned_filename=cleaned_filename,
#             dataset_hash=dataset_hash,
#             rows_original=len(df),
#             rows_cleaned=len(cleaned_df),
#             columns=len(cleaned_df.columns),
#             cleaning_report=cleaning_report,
#             created_at=datetime.utcnow(),
#             committed_to_solana=False
#         )
        
#         await db.insert_metadata(metadata)
        
#         # Update user stats
#         update_user_stats(user_id, file_size)
        
#         # Auto-commit to Solana if requested
#         solana_signature = None
#         if auto_commit:
#             try:
#                 solana_signature = await solana_client.commit_hash(
#                     dataset_hash, user_id, int(datetime.utcnow().timestamp())
#                 )
#                 await db.update_solana_status(dataset_id, True, solana_signature)
#             except Exception as e:
#                 print(f"⚠️ Solana commit failed: {e}")
        
#         # Format file sizes
#         def format_bytes(bytes_val):
#             if bytes_val < 1024:
#                 return f"{bytes_val} B"
#             elif bytes_val < 1024 * 1024:
#                 return f"{bytes_val / 1024:.2f} KB"
#             else:
#                 return f"{bytes_val / (1024 * 1024):.2f} MB"
        
#         return JSONResponse({
#             "success": True,
#             "dataset_id": dataset_id,
#             "dataset_hash": dataset_hash,
#             "original_rows": len(df),
#             "cleaned_rows": len(cleaned_df),
#             "columns": list(cleaned_df.columns),
#             "file_size": format_bytes(file_size),
#             "cleaned_size": format_bytes(cleaned_size),
#             "file_size_bytes": file_size,
#             "cleaned_size_bytes": cleaned_size,
#             "cleaning_report": cleaning_report,
#             "download_url": f"/download/{dataset_id}",
#             "solana_signature": solana_signature,
#             "committed_to_solana": auto_commit and solana_signature is not None
#         })
        
#     except pd.errors.EmptyDataError:
#         raise HTTPException(400, "Empty dataset")
#     except Exception as e:
#         print(f"Upload error: {str(e)}")
#         raise HTTPException(500, f"Processing error: {str(e)}")
    




    # from fastapi import FastAPI, File, UploadFile, HTTPException, Query
# from fastapi.responses import FileResponse, JSONResponse
# from fastapi.middleware.cors import CORSMiddleware
# import pandas as pd
# import hashlib
# import os
# import uuid
# from datetime import datetime
# from typing import Optional
# import io
# from pathlib import Path

# from .cleaning import DataCleaner
# from .blockchain import SolanaClient
# from .database import Database
# from .models import DatasetMetadata, VerifyResponse, HistoryResponse

# app = FastAPI(title="Keginator API", version="1.0.0")

# # CORS for React frontend
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=[
#         "https://keginator.vercel.app",
#         "http://localhost:3000",
#         "*"  # Remove in production
#     ],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # Initialize components
# db = Database()
# cleaner = DataCleaner()
# solana_client = SolanaClient()

# # Storage directories
# UPLOAD_DIR = Path("uploads")
# CLEANED_DIR = Path("cleaned")
# UPLOAD_DIR.mkdir(exist_ok=True)
# CLEANED_DIR.mkdir(exist_ok=True)


# @app.on_event("startup")
# async def startup():
#     """Initialize database and connections"""
#     await db.connect()
#     print("✅ Keginator API Ready")


# @app.on_event("shutdown")
# async def shutdown():
#     """Cleanup connections"""
#     await db.disconnect()


# @app.get("/")
# async def root():
#     return {
#         "message": "Keginator API - Data Cleaning + Solana Blockchain",
#         "version": "1.0.0",
#         "endpoints": ["/upload", "/history/{user_id}", "/commit", "/verify/{hash}"]
#     }


# @app.post("/upload")
# async def upload_dataset(
#     file: UploadFile = File(...),
#     user_id: str = Query(..., description="User identifier"),
#     auto_commit: bool = Query(False, description="Auto-commit to Solana")
# ):
#     """
#     Upload and clean dataset
#     Returns: cleaned file + dataset hash
#     """
#     try:
#         # Validate file type
#         if not file.filename.endswith(('.csv', '.json', '.xlsx', '.xls')):
#             raise HTTPException(400, "Unsupported file format. Use CSV, JSON, or XLSX")
        
#         # Read file content
#         content = await file.read()
        
#         # Load dataset
#         if file.filename.endswith('.csv'):
#             df = pd.read_csv(io.BytesIO(content))
#         elif file.filename.endswith('.json'):
#             df = pd.read_json(io.BytesIO(content))
#         elif file.filename.endswith(('.xlsx', '.xls')):
#             df = pd.read_excel(io.BytesIO(content))
        
#         # Clean dataset
#         cleaned_df, cleaning_report = cleaner.clean(df)
        
#         # Generate unique ID and hash
#         dataset_id = str(uuid.uuid4())
#         cleaned_csv = cleaned_df.to_csv(index=False)
#         dataset_hash = hashlib.sha256(cleaned_csv.encode()).hexdigest()
        
#         # Save cleaned file
#         cleaned_filename = f"{dataset_id}_cleaned.csv"
#         cleaned_path = CLEANED_DIR / cleaned_filename
#         cleaned_df.to_csv(cleaned_path, index=False)
        
#         # Store metadata in database
#         metadata = DatasetMetadata(
#             id=dataset_id,
#             user_id=user_id,
#             original_filename=file.filename,
#             cleaned_filename=cleaned_filename,
#             dataset_hash=dataset_hash,
#             rows_original=len(df),
#             rows_cleaned=len(cleaned_df),
#             columns=len(cleaned_df.columns),
#             cleaning_report=cleaning_report,
#             created_at=datetime.utcnow(),
#             committed_to_solana=False
#         )
        
#         await db.insert_metadata(metadata)
        
#         # Auto-commit to Solana if requested
#         solana_signature = None
#         if auto_commit:
#             try:
#                 solana_signature = await solana_client.commit_hash(
#                     dataset_hash, user_id, int(datetime.utcnow().timestamp())
#                 )
#                 await db.update_solana_status(dataset_id, True, solana_signature)
#             except Exception as e:
#                 print(f"⚠️ Solana commit failed: {e}")
        
#         return JSONResponse({
#             "success": True,
#             "dataset_id": dataset_id,
#             "dataset_hash": dataset_hash,
#             "original_rows": len(df),
#             "cleaned_rows": len(cleaned_df),
#             "columns": list(cleaned_df.columns),
#             "cleaning_report": cleaning_report,
#             "download_url": f"/download/{dataset_id}",
#             "solana_signature": solana_signature,
#             "committed_to_solana": auto_commit and solana_signature is not None
#         })
        
#     except pd.errors.EmptyDataError:
#         raise HTTPException(400, "Empty dataset")
#     except Exception as e:
#         raise HTTPException(500, f"Processing error: {str(e)}")


# @app.get("/download/{dataset_id}")
# async def download_cleaned(dataset_id: str):
#     """Download cleaned dataset"""
#     metadata = await db.get_metadata(dataset_id)
#     if not metadata:
#         raise HTTPException(404, "Dataset not found")
    
#     file_path = CLEANED_DIR / metadata.cleaned_filename
#     if not file_path.exists():
#         raise HTTPException(404, "Cleaned file not found")
    
#     return FileResponse(
#         file_path,
#         media_type="text/csv",
#         filename=metadata.cleaned_filename
#     )


# @app.get("/history/{user_id}")
# async def get_history(user_id: str, limit: int = Query(50, le=100)):
#     """Get user's dataset history"""
#     history = await db.get_user_history(user_id, limit)
    
#     return HistoryResponse(
#         user_id=user_id,
#         total_datasets=len(history),
#         datasets=history
#     )


# @app.post("/commit")
# async def commit_to_solana(
#     dataset_id: str = Query(...),
#     user_id: str = Query(...)
# ):
#     """Commit dataset hash to Solana blockchain"""
#     metadata = await db.get_metadata(dataset_id)
#     if not metadata:
#         raise HTTPException(404, "Dataset not found")
    
#     if metadata.user_id != user_id:
#         raise HTTPException(403, "Unauthorized")
    
#     if metadata.committed_to_solana:
#         return {
#             "success": True,
#             "message": "Already committed",
#             "solana_signature": metadata.solana_signature
#         }
    
#     try:
#         signature = await solana_client.commit_hash(
#             metadata.dataset_hash,
#             user_id,
#             int(metadata.created_at.timestamp())
#         )
        
#         await db.update_solana_status(dataset_id, True, signature)
        
#         return {
#             "success": True,
#             "dataset_hash": metadata.dataset_hash,
#             "solana_signature": signature,
#             "explorer_url": f"https://explorer.solana.com/tx/{signature}?cluster=devnet"
#         }
#     except Exception as e:
#         raise HTTPException(500, f"Solana commit failed: {str(e)}")


# @app.get("/verify/{dataset_hash}")
# async def verify_hash(dataset_hash: str):
#     """Verify if dataset hash exists on Solana"""
#     try:
#         exists, timestamp = await solana_client.verify_hash(dataset_hash)
        
#         return VerifyResponse(
#             dataset_hash=dataset_hash,
#             exists_on_chain=exists,
#             timestamp=timestamp,
#             verified_at=datetime.utcnow()
#         )
#     except Exception as e:
#         raise HTTPException(500, f"Verification failed: {str(e)}")


# @app.get("/health")
# async def health_check():
#     """Health check endpoint"""
#     return {
#         "status": "healthy",
#         "timestamp": datetime.utcnow().isoformat(),
#         "solana_connected": solana_client.is_connected()
#     }


# # from fastapi import FastAPI, File, UploadFile, HTTPException, Query
# # from fastapi.responses import JSONResponse, FileResponse
# # from fastapi.middleware.cors import CORSMiddleware
# # import pandas as pd
# # import hashlib
# # import uuid
# # from datetime import datetime
# # from pathlib import Path
# # import io

# # app = FastAPI(title="Keginator API", version="1.0.0")

# # app.add_middleware(
# #     CORSMiddleware,
# #     allow_origins=["*"],
# #     allow_credentials=True,
# #     allow_methods=["*"],
# #     allow_headers=["*"],
# # )

# # UPLOAD_DIR = Path("uploads")
# # CLEANED_DIR = Path("cleaned")
# # UPLOAD_DIR.mkdir(exist_ok=True)
# # CLEANED_DIR.mkdir(exist_ok=True)

# # @app.get("/")
# # async def root():
# #     return {
# #         "message": "Keginator API - Data Cleaning Engine",
# #         "version": "1.0.0",
# #         "status": "running"
# #     }

# # @app.get("/health")
# # async def health():
# #     return {
# #         "status": "healthy",
# #         "timestamp": datetime.utcnow().isoformat()
# #     }

# # @app.post("/upload")
# # async def upload_dataset(
# #     file: UploadFile = File(...),
# #     user_id: str = Query(..., description="User identifier")
# # ):
# #     try:
# #         if not file.filename.endswith(('.csv', '.json', '.xlsx')):
# #             raise HTTPException(400, "Unsupported file format")
        
# #         content = await file.read()
        
# #         if file.filename.endswith('.csv'):
# #             df = pd.read_csv(io.BytesIO(content))
# #         else:
# #             raise HTTPException(400, "Only CSV supported in demo")
        
# #         # Basic cleaning
# #         original_rows = len(df)
# #         df = df.drop_duplicates()
# #         df = df.dropna(how='all')
# #         cleaned_rows = len(df)
        
# #         # Generate hash
# #         dataset_id = str(uuid.uuid4())
# #         cleaned_csv = df.to_csv(index=False)
# #         dataset_hash = hashlib.sha256(cleaned_csv.encode()).hexdigest()
        
# #         # Save cleaned file
# #         cleaned_filename = f"{dataset_id}_cleaned.csv"
# #         cleaned_path = CLEANED_DIR / cleaned_filename
# #         df.to_csv(cleaned_path, index=False)
        
# #         return JSONResponse({
# #             "success": True,
# #             "dataset_id": dataset_id,
# #             "dataset_hash": dataset_hash,
# #             "original_rows": original_rows,
# #             "cleaned_rows": cleaned_rows,
# #             "columns": list(df.columns),
# #             "download_url": f"/download/{dataset_id}"
# #         })
        
# #     except Exception as e:
# #         raise HTTPException(500, f"Error: {str(e)}")

# # @app.get("/download/{dataset_id}")
# # async def download_cleaned(dataset_id: str):
# #     file_path = CLEANED_DIR / f"{dataset_id}_cleaned.csv"
# #     if not file_path.exists():
# #         raise HTTPException(404, "Dataset not found")
# #     return FileResponse(file_path, media_type="text/csv", filename=f"{dataset_id}_cleaned.csv")



# import pandas as pd
# import numpy as np
# from typing import Tuple, Dict, Any
# from datetime import datetime
# import re
# import os

# try:
#     import google.generativeai as genai
#     GENAI_AVAILABLE = True
# except ImportError:
#     GENAI_AVAILABLE = False
#     print("⚠️ google-generativeai not installed. Install with: pip install google-generativeai")

# class DataCleaner:
#     """
#     AI-Powered data cleaning engine using Gemini
#     Handles missing values, duplicates, type normalization, and formatting
#     """
    
#     def __init__(self):
#         self.report = {}
#         # Initialize Gemini
#         api_key = os.getenv("GEMINI_API_KEY")
#         if api_key:
#             genai.configure(api_key=api_key)
#             self.model = genai.GenerativeModel('gemini-pro')
#             self.ai_enabled = True
#         else:
#             self.ai_enabled = False
#             print("⚠️ GEMINI_API_KEY not set. Using rule-based cleaning only.")
    
#     def clean(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
#         """
#         Main cleaning pipeline with AI-powered analysis
#         Returns: (cleaned_df, cleaning_report)
#         """
#         self.report = {
#             "original_shape": df.shape,
#             "operations": [],
#             "ai_insights": []
#         }
        
#         df_cleaned = df.copy()
        
#         # Step 0: AI-Powered Data Analysis (if enabled)
#         if self.ai_enabled:
#             df_cleaned = self._ai_analyze_and_clean(df_cleaned)
        
#         # Step 1: Remove completely empty rows/columns
#         df_cleaned = self._remove_empty(df_cleaned)
        
#         # Step 2: Standardize column names
#         df_cleaned = self._standardize_columns(df_cleaned)
        
#         # Step 3: Handle duplicates
#         df_cleaned = self._remove_duplicates(df_cleaned)
        
#         # Step 4: Infer and normalize data types
#         df_cleaned = self._normalize_types(df_cleaned)
        
#         # Step 5: Handle missing values
#         df_cleaned = self._handle_missing(df_cleaned)
        
#         # Step 6: Standardize formats (dates, strings, numbers)
#         df_cleaned = self._standardize_formats(df_cleaned)
        
#         # Step 7: Remove outliers (conservative approach)
#         df_cleaned = self._handle_outliers(df_cleaned)
        
#         # Step 8: AI-Powered Final Validation
#         if self.ai_enabled:
#             df_cleaned = self._ai_validate(df_cleaned)
        
#         self.report["final_shape"] = df_cleaned.shape
#         self.report["rows_removed"] = df.shape[0] - df_cleaned.shape[0]
#         self.report["columns_removed"] = df.shape[1] - df_cleaned.shape[1]
        
#         return df_cleaned, self.report
    
#     def _ai_analyze_and_clean(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Use Gemini AI to analyze dataset and suggest cleaning strategies"""
#         try:
#             # Create a summary of the dataset for AI analysis
#             summary = self._create_dataset_summary(df)
            
#             prompt = f"""You are a data cleaning expert. Analyze this dataset summary and provide specific cleaning recommendations in JSON format.

# Dataset Summary:
# {summary}

# Provide recommendations in this exact JSON format (no markdown, just JSON):
# {{
#     "column_types": {{"column_name": "recommended_type"}},
#     "columns_to_drop": ["column1", "column2"],
#     "encoding_fixes": {{"column_name": "fix_description"}},
#     "value_replacements": {{"column_name": {{"old_value": "new_value"}}}},
#     "insights": ["insight1", "insight2"]
# }}"""
            
#             response = self.model.generate_content(prompt)
#             recommendations = self._parse_ai_response(response.text)
            
#             if recommendations:
#                 # Apply AI recommendations
#                 df = self._apply_ai_recommendations(df, recommendations)
#                 self.report["ai_insights"] = recommendations.get("insights", [])
        
#         except Exception as e:
#             print(f"⚠️ AI analysis failed: {e}")
#             self.report["ai_insights"] = ["AI analysis unavailable"]
        
#         return df
    
#     def _create_dataset_summary(self, df: pd.DataFrame) -> str:
#         """Create a concise summary for AI analysis"""
#         summary_parts = []
        
#         # Basic info
#         summary_parts.append(f"Shape: {df.shape[0]} rows × {df.shape[1]} columns")
#         summary_parts.append(f"\nColumns: {', '.join(df.columns.tolist()[:10])}")
        
#         # Data types
#         summary_parts.append(f"\nData Types:\n{df.dtypes.to_string()}")
        
#         # Missing values
#         missing = df.isnull().sum()
#         if missing.any():
#             summary_parts.append(f"\nMissing Values:\n{missing[missing > 0].to_string()}")
        
#         # Sample data (first 3 rows)
#         summary_parts.append(f"\nSample Data (first 3 rows):\n{df.head(3).to_string()}")
        
#         return "\n".join(summary_parts)
    
#     def _parse_ai_response(self, response_text: str) -> Dict[str, Any]:
#         """Parse AI response and extract JSON recommendations"""
#         try:
#             # Remove markdown code blocks if present
#             response_text = re.sub(r'```json\s*', '', response_text)
#             response_text = re.sub(r'```\s*', '', response_text)
            
#             import json
#             recommendations = json.loads(response_text)
#             return recommendations
#         except Exception as e:
#             print(f"⚠️ Failed to parse AI recommendations: {e}")
#             return {}
    
#     def _apply_ai_recommendations(self, df: pd.DataFrame, recommendations: Dict[str, Any]) -> pd.DataFrame:
#         """Apply AI-generated cleaning recommendations"""
#         operations = []
        
#         # Drop recommended columns
#         columns_to_drop = recommendations.get("columns_to_drop", [])
#         if columns_to_drop:
#             existing_cols = [col for col in columns_to_drop if col in df.columns]
#             if existing_cols:
#                 df = df.drop(columns=existing_cols)
#                 operations.append(f"Dropped columns: {', '.join(existing_cols)}")
        
#         # Apply value replacements
#         replacements = recommendations.get("value_replacements", {})
#         for col, replace_map in replacements.items():
#             if col in df.columns:
#                 df[col] = df[col].replace(replace_map)
#                 operations.append(f"Replaced values in {col}")
        
#         # Apply recommended type conversions
#         column_types = recommendations.get("column_types", {})
#         for col, dtype in column_types.items():
#             if col in df.columns:
#                 try:
#                     if dtype == "numeric":
#                         df[col] = pd.to_numeric(df[col], errors='coerce')
#                     elif dtype == "datetime":
#                         df[col] = pd.to_datetime(df[col], errors='coerce')
#                     elif dtype == "string":
#                         df[col] = df[col].astype(str)
#                     operations.append(f"Converted {col} to {dtype}")
#                 except Exception as e:
#                     print(f"⚠️ Failed to convert {col}: {e}")
        
#         if operations:
#             self.report["operations"].append({
#                 "step": "ai_recommendations",
#                 "actions": operations
#             })
        
#         return df
    
#     def _ai_validate(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Final AI validation of cleaned data"""
#         try:
#             summary = self._create_dataset_summary(df)
            
#             prompt = f"""Review this cleaned dataset and identify any remaining data quality issues.

# {summary}

# Provide issues in JSON format:
# {{
#     "quality_score": 85,
#     "remaining_issues": ["issue1", "issue2"],
#     "suggestions": ["suggestion1"]
# }}"""
            
#             response = self.model.generate_content(prompt)
#             validation = self._parse_ai_response(response.text)
            
#             if validation:
#                 self.report["ai_insights"].append(f"Quality Score: {validation.get('quality_score', 'N/A')}")
#                 self.report["ai_insights"].extend(validation.get("suggestions", []))
        
#         except Exception as e:
#             print(f"⚠️ AI validation failed: {e}")
        
#         return df
    
#     def _remove_empty(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Remove completely empty rows and columns"""
#         initial_shape = df.shape
        
#         # Remove empty rows
#         df = df.dropna(how='all')
        
#         # Remove empty columns
#         df = df.dropna(axis=1, how='all')
        
#         removed_rows = initial_shape[0] - df.shape[0]
#         removed_cols = initial_shape[1] - df.shape[1]
        
#         if removed_rows > 0 or removed_cols > 0:
#             self.report["operations"].append({
#                 "step": "remove_empty",
#                 "rows_removed": removed_rows,
#                 "columns_removed": removed_cols
#             })
        
#         return df
    
#     def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Standardize column names: lowercase, underscores, no special chars"""
#         original_cols = df.columns.tolist()
        
#         new_cols = []
#         for col in df.columns:
#             # Convert to string, lowercase
#             col_clean = str(col).lower().strip()
            
#             # Replace spaces and special chars with underscore
#             col_clean = re.sub(r'[^\w\s]', '', col_clean)
#             col_clean = re.sub(r'\s+', '_', col_clean)
            
#             # Remove multiple underscores
#             col_clean = re.sub(r'_+', '_', col_clean)
#             col_clean = col_clean.strip('_')
            
#             new_cols.append(col_clean)
        
#         # Handle duplicate column names
#         seen = {}
#         for i, col in enumerate(new_cols):
#             if col in seen:
#                 seen[col] += 1
#                 new_cols[i] = f"{col}_{seen[col]}"
#             else:
#                 seen[col] = 0
        
#         df.columns = new_cols
        
#         self.report["operations"].append({
#             "step": "standardize_columns",
#             "mapping": dict(zip(original_cols, new_cols))
#         })
        
#         return df
    
#     def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Remove duplicate rows"""
#         initial_rows = len(df)
#         df = df.drop_duplicates()
#         duplicates_removed = initial_rows - len(df)
        
#         if duplicates_removed > 0:
#             self.report["operations"].append({
#                 "step": "remove_duplicates",
#                 "duplicates_removed": duplicates_removed
#             })
        
#         return df
    
#     def _normalize_types(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Infer and normalize column data types"""
#         type_changes = {}
        
#         for col in df.columns:
#             original_type = str(df[col].dtype)
            
#             # Skip if already numeric
#             if pd.api.types.is_numeric_dtype(df[col]):
#                 continue
            
#             # Try converting to numeric
#             if df[col].dtype == 'object':
#                 # Check if it looks like a number
#                 sample = df[col].dropna().head(100)
#                 numeric_count = sum(self._is_numeric_string(str(x)) for x in sample)
                
#                 if len(sample) > 0 and numeric_count / len(sample) > 0.8:
#                     df[col] = pd.to_numeric(df[col], errors='coerce')
#                     type_changes[col] = f"{original_type} -> {df[col].dtype}"
#                     continue
                
#                 # Try converting to datetime
#                 try:
#                     parsed = pd.to_datetime(df[col], errors='coerce')
#                     if parsed.notna().sum() / len(df) > 0.5:
#                         df[col] = parsed
#                         type_changes[col] = f"{original_type} -> datetime64"
#                 except:
#                     pass
        
#         if type_changes:
#             self.report["operations"].append({
#                 "step": "normalize_types",
#                 "type_changes": type_changes
#             })
        
#         return df
    
#     def _is_numeric_string(self, s: str) -> bool:
#         """Check if string represents a number"""
#         try:
#             s = s.strip().replace(',', '').replace('$', '').replace('%', '')
#             float(s)
#             return True
#         except:
#             return False
    
#     def _handle_missing(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Handle missing values intelligently"""
#         missing_strategy = {}
        
#         for col in df.columns:
#             missing_count = df[col].isna().sum()
#             if missing_count == 0:
#                 continue
            
#             missing_pct = missing_count / len(df)
            
#             # Drop column if >50% missing
#             if missing_pct > 0.5:
#                 df = df.drop(columns=[col])
#                 missing_strategy[col] = "dropped (>50% missing)"
#                 continue
            
#             # Numeric columns: fill with median
#             if pd.api.types.is_numeric_dtype(df[col]):
#                 df[col].fillna(df[col].median(), inplace=True)
#                 missing_strategy[col] = "filled with median"
            
#             # Datetime columns: forward fill
#             elif pd.api.types.is_datetime64_any_dtype(df[col]):
#                 df[col].fillna(method='ffill', inplace=True)
#                 missing_strategy[col] = "forward filled"
            
#             # Categorical/string columns: fill with mode or 'Unknown'
#             else:
#                 mode_val = df[col].mode()
#                 if len(mode_val) > 0:
#                     df[col].fillna(mode_val[0], inplace=True)
#                     missing_strategy[col] = "filled with mode"
#                 else:
#                     df[col].fillna('Unknown', inplace=True)
#                     missing_strategy[col] = "filled with 'Unknown'"
        
#         if missing_strategy:
#             self.report["operations"].append({
#                 "step": "handle_missing",
#                 "strategy": missing_strategy
#             })
        
#         return df
    
#     def _standardize_formats(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Standardize string and numeric formats"""
#         format_changes = []
        
#         for col in df.columns:
#             # String columns: trim whitespace
#             if df[col].dtype == 'object':
#                 df[col] = df[col].astype(str).str.strip()
                
#                 # If column looks like a name, apply title case
#                 if 'name' in col.lower():
#                     df[col] = df[col].str.title()
#                     format_changes.append(f"{col}: applied title case")
            
#             # Numeric columns: round floats to 2 decimals
#             elif pd.api.types.is_float_dtype(df[col]):
#                 df[col] = df[col].round(2)
#                 format_changes.append(f"{col}: rounded to 2 decimals")
        
#         if format_changes:
#             self.report["operations"].append({
#                 "step": "standardize_formats",
#                 "changes": format_changes
#             })
        
#         return df
    
#     def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
#         """Remove statistical outliers using IQR method"""
#         outliers_removed = {}
        
#         for col in df.columns:
#             if not pd.api.types.is_numeric_dtype(df[col]):
#                 continue
            
#             Q1 = df[col].quantile(0.25)
#             Q3 = df[col].quantile(0.75)
#             IQR = Q3 - Q1
            
#             # Conservative bounds (3 * IQR instead of 1.5)
#             lower_bound = Q1 - 3 * IQR
#             upper_bound = Q3 + 3 * IQR
            
#             initial_count = len(df)
#             df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
#             removed = initial_count - len(df)
            
#             if removed > 0:
#                 outliers_removed[col] = removed
        
#         if outliers_removed:
#             self.report["operations"].append({
#                 "step": "remove_outliers",
#                 "outliers_removed": outliers_removed
#             })
        
#         return df














        
# # import pandas as pd
# # import numpy as np
# # from typing import Tuple, Dict, Any
# # from datetime import datetime
# # import re


# # class DataCleaner:
# #     """
# #     High-performance data cleaning engine
# #     Handles missing values, duplicates, type normalization, and formatting
# #     """
    
# #     def __init__(self):
# #         self.report = {}
    
# #     def clean(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
# #         """
# #         Main cleaning pipeline
# #         Returns: (cleaned_df, cleaning_report)
# #         """
# #         self.report = {
# #             "original_shape": df.shape,
# #             "operations": []
# #         }
        
# #         df_cleaned = df.copy()
        
# #         # 1. Remove completely empty rows/columns
# #         df_cleaned = self._remove_empty(df_cleaned)
        
# #         # 2. Standardize column names
# #         df_cleaned = self._standardize_columns(df_cleaned)
        
# #         # 3. Handle duplicates
# #         df_cleaned = self._remove_duplicates(df_cleaned)
        
# #         # 4. Infer and normalize data types
# #         df_cleaned = self._normalize_types(df_cleaned)
        
# #         # 5. Handle missing values
# #         df_cleaned = self._handle_missing(df_cleaned)
        
# #         # 6. Standardize formats (dates, strings, numbers)
# #         df_cleaned = self._standardize_formats(df_cleaned)
        
# #         # 7. Remove outliers (optional, conservative approach)
# #         df_cleaned = self._handle_outliers(df_cleaned)
        
# #         self.report["final_shape"] = df_cleaned.shape
# #         self.report["rows_removed"] = df.shape[0] - df_cleaned.shape[0]
# #         self.report["columns_removed"] = df.shape[1] - df_cleaned.shape[1]
        
# #         return df_cleaned, self.report
    
# #     def _remove_empty(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Remove completely empty rows and columns"""
# #         initial_shape = df.shape
        
# #         # Remove empty rows
# #         df = df.dropna(how='all')
        
# #         # Remove empty columns
# #         df = df.dropna(axis=1, how='all')
        
# #         removed_rows = initial_shape[0] - df.shape[0]
# #         removed_cols = initial_shape[1] - df.shape[1]
        
# #         if removed_rows > 0 or removed_cols > 0:
# #             self.report["operations"].append({
# #                 "step": "remove_empty",
# #                 "rows_removed": removed_rows,
# #                 "columns_removed": removed_cols
# #             })
        
# #         return df
    
# #     def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Standardize column names: lowercase, underscores, no special chars"""
# #         original_cols = df.columns.tolist()
        
# #         new_cols = []
# #         for col in df.columns:
# #             # Convert to string, lowercase
# #             col_clean = str(col).lower().strip()
            
# #             # Replace spaces and special chars with underscore
# #             col_clean = re.sub(r'[^\w\s]', '', col_clean)
# #             col_clean = re.sub(r'\s+', '_', col_clean)
            
# #             # Remove multiple underscores
# #             col_clean = re.sub(r'_+', '_', col_clean)
# #             col_clean = col_clean.strip('_')
            
# #             new_cols.append(col_clean)
        
# #         # Handle duplicate column names
# #         seen = {}
# #         for i, col in enumerate(new_cols):
# #             if col in seen:
# #                 seen[col] += 1
# #                 new_cols[i] = f"{col}_{seen[col]}"
# #             else:
# #                 seen[col] = 0
        
# #         df.columns = new_cols
        
# #         self.report["operations"].append({
# #             "step": "standardize_columns",
# #             "mapping": dict(zip(original_cols, new_cols))
# #         })
        
# #         return df
    
# #     def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Remove duplicate rows"""
# #         initial_rows = len(df)
# #         df = df.drop_duplicates()
# #         duplicates_removed = initial_rows - len(df)
        
# #         if duplicates_removed > 0:
# #             self.report["operations"].append({
# #                 "step": "remove_duplicates",
# #                 "duplicates_removed": duplicates_removed
# #             })
        
# #         return df
    
# #     def _normalize_types(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Infer and normalize column data types"""
# #         type_changes = {}
        
# #         for col in df.columns:
# #             original_type = str(df[col].dtype)
            
# #             # Skip if already numeric
# #             if pd.api.types.is_numeric_dtype(df[col]):
# #                 continue
            
# #             # Try converting to numeric
# #             if df[col].dtype == 'object':
# #                 # Check if it looks like a number
# #                 sample = df[col].dropna().head(100)
# #                 numeric_count = sum(self._is_numeric_string(str(x)) for x in sample)
                
# #                 if numeric_count / len(sample) > 0.8:  # 80% threshold
# #                     df[col] = pd.to_numeric(df[col], errors='coerce')
# #                     type_changes[col] = f"{original_type} -> {df[col].dtype}"
# #                     continue
                
# #                 # Try converting to datetime
# #                 try:
# #                     parsed = pd.to_datetime(df[col], errors='coerce')
# #                     if parsed.notna().sum() / len(df) > 0.5:  # 50% threshold
# #                         df[col] = parsed
# #                         type_changes[col] = f"{original_type} -> datetime64"
# #                 except:
# #                     pass
        
# #         if type_changes:
# #             self.report["operations"].append({
# #                 "step": "normalize_types",
# #                 "type_changes": type_changes
# #             })
        
# #         return df
    
# #     def _is_numeric_string(self, s: str) -> bool:
# #         """Check if string represents a number"""
# #         try:
# #             s = s.strip().replace(',', '').replace('$', '').replace('%', '')
# #             float(s)
# #             return True
# #         except:
# #             return False
    
# #     def _handle_missing(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Handle missing values intelligently"""
# #         missing_strategy = {}
        
# #         for col in df.columns:
# #             missing_count = df[col].isna().sum()
# #             if missing_count == 0:
# #                 continue
            
# #             missing_pct = missing_count / len(df)
            
# #             # Drop column if >50% missing
# #             if missing_pct > 0.5:
# #                 df = df.drop(columns=[col])
# #                 missing_strategy[col] = "dropped (>50% missing)"
# #                 continue
            
# #             # Numeric columns: fill with median
# #             if pd.api.types.is_numeric_dtype(df[col]):
# #                 df[col].fillna(df[col].median(), inplace=True)
# #                 missing_strategy[col] = "filled with median"
            
# #             # Datetime columns: forward fill
# #             elif pd.api.types.is_datetime64_any_dtype(df[col]):
# #                 df[col].fillna(method='ffill', inplace=True)
# #                 missing_strategy[col] = "forward filled"
            
# #             # Categorical/string columns: fill with mode or 'Unknown'
# #             else:
# #                 mode_val = df[col].mode()
# #                 if len(mode_val) > 0:
# #                     df[col].fillna(mode_val[0], inplace=True)
# #                     missing_strategy[col] = "filled with mode"
# #                 else:
# #                     df[col].fillna('Unknown', inplace=True)
# #                     missing_strategy[col] = "filled with 'Unknown'"
        
# #         if missing_strategy:
# #             self.report["operations"].append({
# #                 "step": "handle_missing",
# #                 "strategy": missing_strategy
# #             })
        
# #         return df
    
# #     def _standardize_formats(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Standardize string and numeric formats"""
# #         format_changes = []
        
# #         for col in df.columns:
# #             # String columns: trim whitespace, title case for names
# #             if df[col].dtype == 'object':
# #                 df[col] = df[col].astype(str).str.strip()
                
# #                 # If column looks like a name (contains 'name' in column)
# #                 if 'name' in col.lower():
# #                     df[col] = df[col].str.title()
# #                     format_changes.append(f"{col}: applied title case")
            
# #             # Numeric columns: round floats to 2 decimals
# #             elif pd.api.types.is_float_dtype(df[col]):
# #                 df[col] = df[col].round(2)
# #                 format_changes.append(f"{col}: rounded to 2 decimals")
        
# #         if format_changes:
# #             self.report["operations"].append({
# #                 "step": "standardize_formats",
# #                 "changes": format_changes
# #             })
        
# #         return df
    
# #     def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
# #         """Remove statistical outliers (conservative IQR method)"""
# #         outliers_removed = {}
        
# #         for col in df.columns:
# #             if not pd.api.types.is_numeric_dtype(df[col]):
# #                 continue
            
# #             Q1 = df[col].quantile(0.25)
# #             Q3 = df[col].quantile(0.75)
# #             IQR = Q3 - Q1
            
# #             # Conservative bounds (3 * IQR instead of 1.5)
# #             lower_bound = Q1 - 3 * IQR
# #             upper_bound = Q3 + 3 * IQR
            
# #             initial_count = len(df)
# #             df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
# #             removed = initial_count - len(df)
            
# #             if removed > 0:
# #                 outliers_removed[col] = removed
        
# #         if outliers_removed:
# #             self.report["operations"].append({
# #                 "step": "remove_outliers",
# #                 "outliers_removed": outliers_removed
# #             })
        
# #         return df


# # from solana.rpc.async_api import AsyncClient
# # from solders.keypair import Keypair  # Changed from solana.keypair
# # from solana.transaction import Transaction
# # from solders.system_program import transfer, TransferParams  # Changed import
# # from solders.pubkey import Pubkey
# # from solders.instruction import Instruction, AccountMeta
# # from solders.transaction import Transaction as SoldersTransaction  # May need this
# # import base58
# # import os
# # import json
# # from typing import Tuple, Optional
# # import struct


# # class SolanaClient:
# #     """
# #     Solana blockchain client for Keginator
# #     Commits dataset hashes to Solana devnet
# #     """

# #     def __init__(self):
# #         # Use devnet for hackathon
# #         self.rpc_url = os.getenv("SOLANA_RPC_URL", "https://api.devnet.solana.com")
# #         self.client = AsyncClient(self.rpc_url)

# #         # Load payer keypair from env
# #         private_key = os.getenv("SOLANA_PRIVATE_KEY")

# #         if private_key:
# #             try:
# #                 # Case 1: JSON array format (from solana-keygen)
# #                 if private_key.strip().startswith('['):
# #                     key_bytes = bytes(json.loads(private_key))
# #                 else:
# #                     # Case 2: Base58 format (e.g., Phantom export)
# #                     decoded = base58.b58decode(private_key)
# #                     key_bytes = decoded[:64]  # Trim to 64 bytes if longer

# #                 # solders.Keypair expects a 64-byte seed (secret key)
# #                 self.payer = Keypair.from_bytes(key_bytes)
# #             except Exception as e:
# #                 print(f"⚠️ Invalid SOLANA_PRIVATE_KEY format ({e}). Using ephemeral keypair instead.")
# #                 self.payer = Keypair()
# #         else:
# #             # No key provided → ephemeral keypair for testing
# #             self.payer = Keypair()
# #             print("⚠️ Warning: Using ephemeral keypair. Set SOLANA_PRIVATE_KEY in production.")

# #         # Program ID (deployed Anchor program)
# #         program_id_str = os.getenv("KEGINATOR_PROGRAM_ID", "11111111111111111111111111111111")
# #         self.program_id = Pubkey.from_string(program_id_str)

# #         print(f"🔗 Solana client initialized: {self.rpc_url}")
# #         print(f"💼 Payer: {self.payer.pubkey()}")

# #     def is_connected(self) -> bool:
# #         """Check if connected to Solana"""
# #         try:
# #             return self.client is not None
# #         except:
# #             return False

# #     async def commit_hash(
# #         self,
# #         dataset_hash: str,
# #         user_id: str,
# #         timestamp: int
# #     ) -> str:
# #         """
# #         Commit dataset hash to Solana blockchain
# #         Returns: transaction signature
# #         """
# #         try:
# #             dataset_pda, bump = self._get_dataset_pda(dataset_hash)

# #             # Build instruction data
# #             instruction_data = self._build_commit_instruction(
# #                 dataset_hash, user_id, timestamp, bump
# #             )

# #             # Create instruction
# #             instruction = Instruction(
# #                 program_id=self.program_id,
# #                 accounts=[
# #                     AccountMeta(pubkey=dataset_pda, is_signer=False, is_writable=True),
# #                     AccountMeta(pubkey=self.payer.pubkey(), is_signer=True, is_writable=True),
# #                     AccountMeta(pubkey=Pubkey.from_string("11111111111111111111111111111111"), is_signer=False, is_writable=False),
# #                 ],
# #                 data=instruction_data,
# #             )

# #             from solders.message import Message
# #             from solders.transaction import Transaction as SoldersTransaction

# #             blockhash_resp = await self.client.get_latest_blockhash()
# #             blockhash = blockhash_resp.value.blockhash

# #             message = Message.new_with_blockhash(
# #                 [instruction],
# #                 self.payer.pubkey(),
# #                 blockhash
# #             )

# #             transaction = SoldersTransaction.new_unsigned(message)
# #             transaction = SoldersTransaction([self.payer], message, blockhash)

# #             response = await self.client.send_transaction(transaction)
# #             signature = str(response.value)

# #             await self.client.confirm_transaction(signature)

# #             print(f"✅ Hash committed to Solana: {signature}")
# #             return signature

# #         except Exception as e:
# #             print(f"❌ Solana commit failed: {e}")
# #             raise Exception(f"Failed to commit to Solana: {str(e)}")

# #     async def verify_hash(self, dataset_hash: str) -> Tuple[bool, Optional[int]]:
# #         """
# #         Verify if dataset hash exists on Solana
# #         Returns: (exists, timestamp)
# #         """
# #         try:
# #             dataset_pda, _ = self._get_dataset_pda(dataset_hash)
# #             account_info = await self.client.get_account_info(dataset_pda)

# #             if account_info.value is None:
# #                 return False, None

# #             data = account_info.value.data
# #             if len(data) >= 81:
# #                 timestamp = struct.unpack('<Q', data[72:80])[0]
# #                 return True, timestamp

# #             return True, None

# #         except Exception as e:
# #             print(f"⚠️ Verification error: {e}")
# #             return False, None

# #     def _get_dataset_pda(self, dataset_hash: str) -> Tuple[Pubkey, int]:
# #         seeds = [
# #             b"dataset",
# #             dataset_hash.encode()[:32]
# #         ]
# #         pda, bump = Pubkey.find_program_address(seeds, self.program_id)
# #         return pda, bump

# #     def _build_commit_instruction(
# #         self,
# #         dataset_hash: str,
# #         user_id: str,
# #         timestamp: int,
# #         bump: int
# #     ) -> bytes:
# #         discriminator = bytes([174, 39, 232, 127, 252, 3, 250, 87])

# #         hash_bytes = dataset_hash.encode()[:32].ljust(32, b'\0')
# #         user_bytes = user_id.encode()[:32].ljust(32, b'\0')
# #         timestamp_bytes = struct.pack('<Q', timestamp)
# #         bump_bytes = bytes([bump])

# #         return discriminator + hash_bytes + user_bytes + timestamp_bytes + bump_bytes

# #     async def get_balance(self) -> float:
# #         """Get payer account balance in SOL"""
# #         try:
# #             balance = await self.client.get_balance(self.payer.pubkey())
# #             return balance.value / 1e9
# #         except:
# #             return 0.0

# #     async def close(self):
# #         """Close client connection"""
# #         await self.client.close()


# # # Utility function for testing
# # async def test_connection():
# #     client = SolanaClient()
# #     try:
# #         balance = await client.get_balance()
# #         print(f"✅ Connected to Solana")
# #         print(f"💰 Balance: {balance} SOL")
# #         return True
# #     except Exception as e:
# #         print(f"❌ Connection failed: {e}")
# #         return False
# #     finally:
# #         await client.close()


# # if __name__ == "__main__":
# #     import asyncio
# #     asyncio.run(test_connection())




# from solana.rpc.async_api import AsyncClient
# from solders.keypair import Keypair
# from solana.transaction import Transaction
# from solders.system_program import transfer, TransferParams
# from solders.pubkey import Pubkey
# from solders.instruction import Instruction, AccountMeta
# from solders.transaction import Transaction as SoldersTransaction
# import base58
# import os
# import json
# from typing import Tuple, Optional
# import struct


# class SolanaClient:
#     """
#     Solana blockchain client for Keginator
#     Commits dataset hashes to Solana devnet
#     """

#     def __init__(self):
#         self.rpc_url = os.getenv("SOLANA_RPC_URL", "https://api.devnet.solana.com")
#         self.client = AsyncClient(self.rpc_url)

#         # Load payer keypair from env
#         private_key = os.getenv("SOLANA_PRIVATE_KEY")

#         if private_key:
#             try:
#                 # Case 1: JSON array format (from solana-keygen)
#                 if private_key.strip().startswith('['):
#                     key_bytes = bytes(json.loads(private_key))
#                     # Trim to 64 bytes if needed
#                     if len(key_bytes) > 64:
#                         key_bytes = key_bytes[:64]
#                 else:
#                     # Case 2: Base58 format (e.g., Phantom export)
#                     decoded = base58.b58decode(private_key)
#                     key_bytes = decoded[:64]

#                 # FIXED: Use from_bytes (not from_secret_key)
#                 self.payer = Keypair.from_bytes(key_bytes)
                
#             except Exception as e:
#                 print(f"⚠️ Invalid SOLANA_PRIVATE_KEY format ({e}). Using ephemeral keypair instead.")
#                 self.payer = Keypair()
#         else:
#             self.payer = Keypair()
#             print("⚠️ Warning: Using ephemeral keypair. Set SOLANA_PRIVATE_KEY in production.")

#         # Program ID (deployed Anchor program)
#         program_id_str = os.getenv("KEGINATOR_PROGRAM_ID", "11111111111111111111111111111111")
#         self.program_id = Pubkey.from_string(program_id_str)

#         print(f"🔗 Solana client initialized: {self.rpc_url}")
#         print(f"💼 Payer: {self.payer.pubkey()}")

#     def is_connected(self) -> bool:
#         """Check if connected to Solana"""
#         try:
#             return self.client is not None
#         except:
#             return False

#     async def commit_hash(
#         self,
#         dataset_hash: str,
#         user_id: str,
#         timestamp: int
#     ) -> str:
#         """
#         Commit dataset hash to Solana blockchain
#         Returns: transaction signature
#         """
#         try:
#             dataset_pda, bump = self._get_dataset_pda(dataset_hash)

#             instruction_data = self._build_commit_instruction(
#                 dataset_hash, user_id, timestamp, bump
#             )

#             instruction = Instruction(
#                 program_id=self.program_id,
#                 accounts=[
#                     AccountMeta(pubkey=dataset_pda, is_signer=False, is_writable=True),
#                     AccountMeta(pubkey=self.payer.pubkey(), is_signer=True, is_writable=True),
#                     AccountMeta(pubkey=Pubkey.from_string("11111111111111111111111111111111"), is_signer=False, is_writable=False),
#                 ],
#                 data=instruction_data,
#             )

#             from solders.message import Message
#             from solders.transaction import Transaction as SoldersTransaction

#             blockhash_resp = await self.client.get_latest_blockhash()
#             blockhash = blockhash_resp.value.blockhash

#             message = Message.new_with_blockhash(
#                 [instruction],
#                 self.payer.pubkey(),
#                 blockhash
#             )

#             transaction = SoldersTransaction.new_unsigned(message)
#             transaction = SoldersTransaction([self.payer], message, blockhash)

#             response = await self.client.send_transaction(transaction)
#             signature = str(response.value)

#             await self.client.confirm_transaction(signature)

#             print(f"✅ Hash committed to Solana: {signature}")
#             return signature

#         except Exception as e:
#             print(f"❌ Solana commit failed: {e}")
#             raise Exception(f"Failed to commit to Solana: {str(e)}")

#     async def verify_hash(self, dataset_hash: str) -> Tuple[bool, Optional[int]]:
#         """
#         Verify if dataset hash exists on Solana
#         Returns: (exists, timestamp)
#         """
#         try:
#             dataset_pda, _ = self._get_dataset_pda(dataset_hash)
#             account_info = await self.client.get_account_info(dataset_pda)

#             if account_info.value is None:
#                 return False, None

#             data = account_info.value.data
#             if len(data) >= 81:
#                 timestamp = struct.unpack('<Q', data[72:80])[0]
#                 return True, timestamp

#             return True, None

#         except Exception as e:
#             print(f"⚠️ Verification error: {e}")
#             return False, None

#     def _get_dataset_pda(self, dataset_hash: str) -> Tuple[Pubkey, int]:
#         seeds = [
#             b"dataset",
#             dataset_hash.encode()[:32]
#         ]
#         pda, bump = Pubkey.find_program_address(seeds, self.program_id)
#         return pda, bump

#     def _build_commit_instruction(
#         self,
#         dataset_hash: str,
#         user_id: str,
#         timestamp: int,
#         bump: int
#     ) -> bytes:
#         discriminator = bytes([174, 39, 232, 127, 252, 3, 250, 87])

#         hash_bytes = dataset_hash.encode()[:32].ljust(32, b'\0')
#         user_bytes = user_id.encode()[:32].ljust(32, b'\0')
#         timestamp_bytes = struct.pack('<Q', timestamp)
#         bump_bytes = bytes([bump])

#         return discriminator + hash_bytes + user_bytes + timestamp_bytes + bump_bytes

#     async def get_balance(self) -> float:
#         """Get payer account balance in SOL"""
#         try:
#             balance = await self.client.get_balance(self.payer.pubkey())
#             return balance.value / 1e9
#         except:
#             return 0.0

#     async def close(self):
#         """Close client connection"""
#         await self.client.close()


# async def test_connection():
#     client = SolanaClient()
#     try:
#         balance = await client.get_balance()
#         print(f"✅ Connected to Solana")
#         print(f"💰 Balance: {balance} SOL")
#         return True
#     except Exception as e:
#         print(f"❌ Connection failed: {e}")
#         return False
#     finally:
#         await client.close()


# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(test_connection())

